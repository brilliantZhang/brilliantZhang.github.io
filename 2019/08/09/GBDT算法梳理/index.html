<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="前向分布算法前向分布算法(forward stagewise algorithm)求解优化问题的思想：因为学习的是加法模型，那如果能够从前向后，每一步只学习一个基函数及其系数，然后逐步逼近优化目标，那么就可以简化优化的复杂度。
负梯度拟合GBDT在函数空间中利用梯度下降法进行优化。即GDBT在优化中使用了一阶导数信息,GBDT是从负梯度的方向去拟合改进模型。GBDT用损失函数的负梯度来拟合本轮损失">
<meta property="og:type" content="article">
<meta property="og:title" content="GBDT算法梳理">
<meta property="og:url" content="brilliantZhang.github.io/2019/08/09/GBDT算法梳理/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="前向分布算法前向分布算法(forward stagewise algorithm)求解优化问题的思想：因为学习的是加法模型，那如果能够从前向后，每一步只学习一个基函数及其系数，然后逐步逼近优化目标，那么就可以简化优化的复杂度。
负梯度拟合GBDT在函数空间中利用梯度下降法进行优化。即GDBT在优化中使用了一阶导数信息,GBDT是从负梯度的方向去拟合改进模型。GBDT用损失函数的负梯度来拟合本轮损失">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/6.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/7.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/8.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/9.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/10.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/11.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/12.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/13.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/14.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/15.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/16.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/17.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/18.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/19.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/20.PNG">
<meta property="og:image" content="brilliantZhang.github.io/machine_learning/21.PNG">
<meta property="og:updated_time" content="2019-08-09T12:36:06.318Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GBDT算法梳理">
<meta name="twitter:description" content="前向分布算法前向分布算法(forward stagewise algorithm)求解优化问题的思想：因为学习的是加法模型，那如果能够从前向后，每一步只学习一个基函数及其系数，然后逐步逼近优化目标，那么就可以简化优化的复杂度。
负梯度拟合GBDT在函数空间中利用梯度下降法进行优化。即GDBT在优化中使用了一阶导数信息,GBDT是从负梯度的方向去拟合改进模型。GBDT用损失函数的负梯度来拟合本轮损失">
<meta name="twitter:image" content="brilliantZhang.github.io/machine_learning/6.PNG">






  <link rel="canonical" href="brilliantZhang.github.io/2019/08/09/GBDT算法梳理/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>GBDT算法梳理 | My Blog</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">My Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">So we beat on, boats against the current, borne back ceaselessly into the past.</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="brilliantZhang.github.io/2019/08/09/GBDT算法梳理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brilliantZhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">GBDT算法梳理
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-09 20:32:51 / 修改时间：20:36:06" itemprop="dateCreated datePublished" datetime="2019-08-09T20:32:51+08:00">2019-08-09</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="前向分布算法"><a href="#前向分布算法" class="headerlink" title="前向分布算法"></a>前向分布算法</h1><p>前向分布算法(forward stagewise algorithm)求解优化问题的思想：<br>因为学习的是加法模型，那如果能够从前向后，每一步只学习一个基函数及其系数，然后逐步逼近优化目标，那么就可以简化优化的复杂度。<br><img src="/machine_learning/6.PNG" alt=""><br><img src="/machine_learning/7.PNG" alt=""></p>
<h1 id="负梯度拟合"><a href="#负梯度拟合" class="headerlink" title="负梯度拟合"></a>负梯度拟合</h1><p>GBDT在函数空间中利用梯度下降法进行优化。即GDBT在优化中使用了一阶导数信息,GBDT是从负梯度的方向去拟合改进模型。<br>GBDT用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。其模型 F 定义为加法模型：<br><img src="/machine_learning/8.PNG" alt=""></p>
<p>其中，x为输入样本，h为分类回归树，w是分类回归树的参数，α是每棵树的权重。<br>通过最小化损失函数求解最优模型：<br><img src="/machine_learning/9.PNG" alt=""></p>
<h2 id="Gradient-Boosting-Tree-算法原理"><a href="#Gradient-Boosting-Tree-算法原理" class="headerlink" title="Gradient Boosting Tree 算法原理"></a>Gradient Boosting Tree 算法原理</h2><p><img src="/machine_learning/10.PNG" alt=""></p>
<p>算法步骤<br>1）明确损失函数是误差最小</p>
<p>2）构建第一棵回归树</p>
<p>3）学习多棵回归树<br><img src="/machine_learning/11.PNG" alt=""><br>迭代：计算梯度/残差gm(如果是均方误差为损失函数即为残差)</p>
<p>步长/缩放因子p,用 a single Newton-Raphson step 去近似求解下降方向步长，通常的实现中 Step3 被省略，采用 shrinkage 的策略通过参数设置步长，避免过拟合。</p>
<p>4）F(x)等于所有树结果累加<br><img src="/machine_learning/12.PNG" alt=""></p>
<h1 id="二分类，多分类"><a href="#二分类，多分类" class="headerlink" title="二分类，多分类"></a>二分类，多分类</h1><p>为了解决分类问题中样本输出不是连续的值，而是离散的类别，无法直接从输出类别去拟合类别输出的误差这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法；另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。对于对数似然损失函数，我们又有二元分类和多元分类的区别。</p>
<h2 id="二元GBDT分类算法"><a href="#二元GBDT分类算法" class="headerlink" title="二元GBDT分类算法"></a>二元GBDT分类算法</h2><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：<br><img src="/machine_learning/13.PNG" alt=""></p>
<p>其中y∈{−1,+1}。则此时的负梯度误差为：<br><img src="/machine_learning/14.PNG" alt=""></p>
<p>对于生成的决策树，各个叶子节点的最佳负梯度拟合值为：<br><img src="/machine_learning/15.PNG" alt=""></p>
<p>由于上式比较难优化，我们一般使用近似值代替：<br><img src="/machine_learning/16.PNG" alt=""></p>
<h2 id="多元GBDT分类算法"><a href="#多元GBDT分类算法" class="headerlink" title="多元GBDT分类算法"></a>多元GBDT分类算法</h2><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：<br><img src="/machine_learning/17.PNG" alt=""></p>
<p>其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为：<br><img src="/machine_learning/18.PNG" alt=""></p>
<p>我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为：<br><img src="/machine_learning/19.PNG" alt=""></p>
<p>误差就是样本i对应类别l的真实概率和t−1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为：<br><img src="/machine_learning/20.PNG" alt=""></p>
<p>一般使用近似值代替<br><img src="/machine_learning/21.PNG" alt=""></p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>GBDT的正则化主要有三种方式。</p>
<p>第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν,对于前面的弱学习器的迭代<br>fk(x)=fk−1(x)+hk(x)<br>如果我们加上了正则化项，则有<br>fk(x)=fk−1(x)+νhk(x)<br>ν的取值范围为0&lt;ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<p>第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。<br>使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p>
<p>第三种是对于弱学习器即CART回归树进行正则化剪枝。</p>
<h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p>GBDT主要的优点有：<br>1) 可以灵活处理各种类型的数据，包括连续值和离散值。<br>2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。<br>3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p>
<p>GBDT的主要缺点有：<br>1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p>
<h1 id="sklearn参数"><a href="#sklearn参数" class="headerlink" title="sklearn参数"></a>sklearn参数</h1><h2 id="GBDT类库boosting框架参数"><a href="#GBDT类库boosting框架参数" class="headerlink" title="GBDT类库boosting框架参数"></a>GBDT类库boosting框架参数</h2><p>　　　　<br>1) n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。</p>
<p>2) learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长，加上了正则化项，强学习器的迭代公式为fk(x)=fk−1(x)+νhk(x)。ν的取值范围为0&lt;ν≤1。对于同样的训练集拟合效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的ν开始调参，默认是1。</p>
<p>3) subsample: 子采样，取值为(0,1]。这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</p>
<p>4) init: 即我们的初始化的时候的弱学习器，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。</p>
<p>5) loss: 即GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</p>
<p>6) alpha：这个参数只有GradientBoostingRegressor有，当我们使用Huber损失”huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。</p>
<h2 id="GBDT类库弱学习器参数"><a href="#GBDT类库弱学习器参数" class="headerlink" title="GBDT类库弱学习器参数"></a>GBDT类库弱学习器参数</h2><p>1) 划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑log2N个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑根号N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p>
<p>2) 决策树最大深度max_depth: 默认可以不输入，如果不输入的话，默认值是3。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p>
<p>3) 内部节点再划分所需最小样本数min_samples_split: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p>
<p>4) 叶子节点最少样本数min_samples_leaf:这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p>
<p>5）叶子节点最小的样本权重和min_weight_fraction_leaf：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p>
<p>6) 最大叶子节点数max_leaf_nodes: 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p>
<p>7) 节点划分最小不纯度min_impurity_split:  这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。</p>
<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><p>GBDT几乎可用于所有回归问题（线性/非线性），GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。</p>
<h1 id="随机森林和-GBDT-的区别"><a href="#随机森林和-GBDT-的区别" class="headerlink" title="随机森林和 GBDT 的区别"></a>随机森林和 GBDT 的区别</h1><p>1）随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。</p>
<p>2）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。</p>
<p>3）组成随机森林的树可以并行生成；而GBDT只能是串行生成。</p>
<p>4）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。</p>
<p>5）随机森林对异常值不敏感；GBDT对异常值非常敏感。</p>
<p>6）随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。</p>
<p>7）随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/07/Leetcode-买卖股票的最佳时机II/" rel="next" title="Leetcode:买卖股票的最佳时机II">
                <i class="fa fa-chevron-left"></i> Leetcode:买卖股票的最佳时机II
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/10/排序链表/" rel="prev" title="排序链表">
                排序链表 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">brilliantZhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">32</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前向分布算法"><span class="nav-number">1.</span> <span class="nav-text">前向分布算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#负梯度拟合"><span class="nav-number">2.</span> <span class="nav-text">负梯度拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Boosting-Tree-算法原理"><span class="nav-number">2.1.</span> <span class="nav-text">Gradient Boosting Tree 算法原理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二分类，多分类"><span class="nav-number">3.</span> <span class="nav-text">二分类，多分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#二元GBDT分类算法"><span class="nav-number">3.1.</span> <span class="nav-text">二元GBDT分类算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多元GBDT分类算法"><span class="nav-number">3.2.</span> <span class="nav-text">多元GBDT分类算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#正则化"><span class="nav-number">4.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#优缺点"><span class="nav-number">5.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sklearn参数"><span class="nav-number">6.</span> <span class="nav-text">sklearn参数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT类库boosting框架参数"><span class="nav-number">6.1.</span> <span class="nav-text">GBDT类库boosting框架参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT类库弱学习器参数"><span class="nav-number">6.2.</span> <span class="nav-text">GBDT类库弱学习器参数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#应用场景"><span class="nav-number">7.</span> <span class="nav-text">应用场景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#随机森林和-GBDT-的区别"><span class="nav-number">8.</span> <span class="nav-text">随机森林和 GBDT 的区别</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">brilliantZhang</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.2.2</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



	





  





  










  





  

  

  

  

  
  

  

  

  

  

  

</body>
</html>
